
Open In Colab
Table of Contents
1  Loading the MNIST dataset in Keras
2  The network architecture
3  The compilation step
4  Preparing the image data
5  Preparing the labels
6  Training and Testing
Loading the MNIST dataset in Keras
In [0]:
from keras.datasets import mnist
Using TensorFlow backend.
The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.
We recommend you upgrade now or ensure your notebook will continue to use TensorFlow 1.x via the %tensorflow_version 1.x magic: more info.

In [0]:
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz
11493376/11490434 [==============================] - 1s 0us/step
The images are encoded as Numpy arrays, and the labels are an array of digits, ranging from 0 to 9.
In [0]:
train_images.shape
Out[0]:
(60000, 28, 28)
In [0]:
len(train_labels)
Out[0]:
60000
In [0]:
train_labels
Out[0]:
array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)
In [0]:
test_images.shape
Out[0]:
(10000, 28, 28)
In [0]:
len(test_labels)
Out[0]:
10000
In [0]:
test_labels
Out[0]:
array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)
Let's build the network

The network architecture
The core building block of neural networks is the layer, a data-processing module that you can think of as a filter for data.

Some data goes in, and it comes out in a more useful form.
Layers extract representations (hopefully, meaningful for the data problem at hand) out of the data fed into them.
Most of deep learning consists of chaining together simple layers that will implement a form of progressive data distillation.

A deep learning model is like a sieve for data-processing, made of a succession of increasingly refined data filters--the layers.
In [0]:
from keras import models
from keras import layers
In [0]:
network = models.Sequential()
# Dense(512) is a fully-connected layer with 512 hidden units.
# in the first layer, you must specify the expected input data shape :
# here, 28 X 28=784 -dimensional vectors.
network.add(layers.Dense(612, activation='relu', input_shape=(28 * 28, )))
network.add(layers.Dense(350, activation='relu', input_shape=(28 * 28, )))
network.add(layers.Dense(10, activation='softmax'))
Our network consists of a sequence of two Dense layers, which are densely connected (also called fully connected) neural layers.
The second (and last) layer is a 10-way softmax layer, which means it will return an array of 10 probability scores. Each score will be the probability that the current digit image belongs to one of our 10 digit classes.
The compilation step
To make the network ready for training, we need to pick three more things, as part of the compilation step:
A loss function-- How the network will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction.
An optimizer--The mechanism through which the network will update itself based on the data it sees and its loss function.
Metrics to monitor during training and testing--Here, we will only care about accuracy (the fraction of the images that were correctly classified).
In [0]:
network.compile(optimizer='rmsprop',
                loss='categorical_crossentropy',
                metrics=['accuracy'])
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.

Preparing the image data
Before training, we will preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the $[0-1]$ interval.

In [0]:
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype('float32') / 255.
In [0]:
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype('float32') / 255.
Preparing the labels
We also need to categorically encode the labels.

In [0]:
from keras.utils import to_categorical
In [0]:
train_labels = to_categorical(train_labels)
train_labels
Out[0]:
array([[0., 0., 0., ..., 0., 0., 0.],
       [1., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)
In [0]:
test_labels = to_categorical(test_labels)
test_labels
Out[0]:
array([[0., 0., 0., ..., 1., 0., 0.],
       [0., 0., 1., ..., 0., 0., 0.],
       [0., 1., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)
Training and Testing
We are now ready to train the network, which in Keras is done via a call to the network's fit method--we fit the model to its training data:

In [0]:
network.fit(train_images, train_labels, epochs=45, batch_size=30)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

Epoch 1/45
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

60000/60000 [==============================] - 22s 371us/step - loss: 0.1965 - acc: 0.9433
Epoch 2/45
60000/60000 [==============================] - 21s 358us/step - loss: 0.1015 - acc: 0.9741
Epoch 3/45
60000/60000 [==============================] - 22s 364us/step - loss: 0.0845 - acc: 0.9804
Epoch 4/45
60000/60000 [==============================] - 22s 361us/step - loss: 0.0758 - acc: 0.9836
Epoch 5/45
60000/60000 [==============================] - 21s 354us/step - loss: 0.0681 - acc: 0.9859
Epoch 6/45
60000/60000 [==============================] - 21s 356us/step - loss: 0.0632 - acc: 0.9877
Epoch 7/45
60000/60000 [==============================] - 21s 353us/step - loss: 0.0631 - acc: 0.9891
Epoch 8/45
60000/60000 [==============================] - 21s 349us/step - loss: 0.0570 - acc: 0.9902
Epoch 9/45
60000/60000 [==============================] - 21s 354us/step - loss: 0.0496 - acc: 0.9919
Epoch 10/45
60000/60000 [==============================] - 21s 354us/step - loss: 0.0483 - acc: 0.9921
Epoch 11/45
60000/60000 [==============================] - 21s 356us/step - loss: 0.0457 - acc: 0.9931
Epoch 12/45
60000/60000 [==============================] - 21s 352us/step - loss: 0.0451 - acc: 0.9936
Epoch 13/45
60000/60000 [==============================] - 21s 344us/step - loss: 0.0419 - acc: 0.9937
Epoch 14/45
60000/60000 [==============================] - 21s 344us/step - loss: 0.0357 - acc: 0.9950
Epoch 15/45
60000/60000 [==============================] - 21s 345us/step - loss: 0.0333 - acc: 0.9951
Epoch 16/45
60000/60000 [==============================] - 21s 346us/step - loss: 0.0332 - acc: 0.9956
Epoch 17/45
60000/60000 [==============================] - 21s 342us/step - loss: 0.0298 - acc: 0.9959
Epoch 18/45
60000/60000 [==============================] - 21s 351us/step - loss: 0.0298 - acc: 0.9962
Epoch 19/45
60000/60000 [==============================] - 21s 342us/step - loss: 0.0320 - acc: 0.9960
Epoch 20/45
60000/60000 [==============================] - 21s 345us/step - loss: 0.0287 - acc: 0.9962
Epoch 21/45
60000/60000 [==============================] - 21s 347us/step - loss: 0.0258 - acc: 0.9970
Epoch 22/45
60000/60000 [==============================] - 20s 339us/step - loss: 0.0251 - acc: 0.9970
Epoch 23/45
60000/60000 [==============================] - 20s 339us/step - loss: 0.0241 - acc: 0.9971
Epoch 24/45
60000/60000 [==============================] - 20s 334us/step - loss: 0.0222 - acc: 0.9974
Epoch 25/45
60000/60000 [==============================] - 20s 336us/step - loss: 0.0273 - acc: 0.9968
Epoch 26/45
60000/60000 [==============================] - 20s 341us/step - loss: 0.0218 - acc: 0.9974
Epoch 27/45
60000/60000 [==============================] - 20s 337us/step - loss: 0.0230 - acc: 0.9974
Epoch 28/45
60000/60000 [==============================] - 20s 340us/step - loss: 0.0221 - acc: 0.9974
Epoch 29/45
60000/60000 [==============================] - 21s 349us/step - loss: 0.0199 - acc: 0.9979
Epoch 30/45
60000/60000 [==============================] - 21s 344us/step - loss: 0.0207 - acc: 0.9979
Epoch 31/45
60000/60000 [==============================] - 21s 351us/step - loss: 0.0230 - acc: 0.9976
Epoch 32/45
60000/60000 [==============================] - 21s 346us/step - loss: 0.0223 - acc: 0.9978
Epoch 33/45
60000/60000 [==============================] - 20s 339us/step - loss: 0.0206 - acc: 0.9980
Epoch 34/45
60000/60000 [==============================] - 20s 339us/step - loss: 0.0193 - acc: 0.9979
Epoch 35/45
60000/60000 [==============================] - 20s 337us/step - loss: 0.0205 - acc: 0.9979
Epoch 36/45
60000/60000 [==============================] - 20s 334us/step - loss: 0.0203 - acc: 0.9980
Epoch 37/45
60000/60000 [==============================] - 20s 331us/step - loss: 0.0184 - acc: 0.9979
Epoch 38/45
60000/60000 [==============================] - 20s 337us/step - loss: 0.0158 - acc: 0.9985
Epoch 39/45
60000/60000 [==============================] - 20s 331us/step - loss: 0.0179 - acc: 0.9983
Epoch 40/45
60000/60000 [==============================] - 20s 334us/step - loss: 0.0160 - acc: 0.9985
Epoch 41/45
60000/60000 [==============================] - 20s 340us/step - loss: 0.0161 - acc: 0.9984
Epoch 42/45
60000/60000 [==============================] - 20s 337us/step - loss: 0.0158 - acc: 0.9985
Epoch 43/45
60000/60000 [==============================] - 20s 335us/step - loss: 0.0122 - acc: 0.9987
Epoch 44/45
60000/60000 [==============================] - 21s 342us/step - loss: 0.0149 - acc: 0.9985
Epoch 45/45
60000/60000 [==============================] - 20s 338us/step - loss: 0.0124 - acc: 0.9988
Out[0]:
<keras.callbacks.History at 0x7f4222324588>
Two quantities are displayed during training:

The loss of the network over the training data
The accuracy of the network over the training data
We quickly reach an accuracy of $0.9886 (98.86\%)$ on the training data.

Now let's check that the model performs well on the test set, too:
In [0]:
test_loss, test_acc = network.evaluate(test_images, test_labels)
10000/10000 [==============================] - 1s 52us/step
In [0]:
print('Test Accuracy: {:.5f} '.format(test_acc))
Test Accuracy: 0.97880 
In [0]:
test_acc
Out[0]:
0.9788
The test-set accuracy turns out to be $97.780\%$--that is quite a bit lower than the training set accuracy. This gap between training and test accuracy is an example of overfitting:the fact that the ML models tend to perform worse on new data than on their training data.
In [0]:
%load_ext version_information
%version_information keras, numpy
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-48-559039289680> in <module>()
----> 1 get_ipython().magic('load_ext version_information')
      2 get_ipython().magic('version_information keras, numpy')

/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py in magic(self, arg_s)
   2158         magic_name, _, magic_arg_s = arg_s.partition(' ')
   2159         magic_name = magic_name.lstrip(prefilter.ESC_MAGIC)
-> 2160         return self.run_line_magic(magic_name, magic_arg_s)
   2161 
   2162     #-------------------------------------------------------------------------

/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py in run_line_magic(self, magic_name, line)
   2079                 kwargs['local_ns'] = sys._getframe(stack_depth).f_locals
   2080             with self.builtin_trap:
-> 2081                 result = fn(*args,**kwargs)
   2082             return result
   2083 

</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-63> in load_ext(self, module_str)

/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py in <lambda>(f, *a, **k)
    186     # but it's overkill for just that one bit of state.
    187     def magic_deco(arg):
--> 188         call = lambda f, *a, **k: f(*a, **k)
    189 
    190         if callable(arg):

/usr/local/lib/python3.6/dist-packages/IPython/core/magics/extension.py in load_ext(self, module_str)
     35         if not module_str:
     36             raise UsageError('Missing module name.')
---> 37         res = self.shell.extension_manager.load_extension(module_str)
     38 
     39         if res == 'already loaded':

/usr/local/lib/python3.6/dist-packages/IPython/core/extensions.py in load_extension(self, module_str)
     81             if module_str not in sys.modules:
     82                 with prepended_to_syspath(self.ipython_extension_dir):
---> 83                     __import__(module_str)
     84             mod = sys.modules[module_str]
     85             if self._call_load_ipython_extension(mod):

ModuleNotFoundError: No module named 'version_information'

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
"Open Examples" button below.
---------------------------------------------------------------------------
